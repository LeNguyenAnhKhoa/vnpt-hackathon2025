import requests
from bs4 import BeautifulSoup
import json
import re

def clean_text(text):
    """
    H√†m l√†m s·∫°ch vƒÉn b·∫£n:
    - X√≥a kho·∫£ng tr·∫Øng th·ª´a ƒë·∫ßu ƒëu√¥i.
    - X√≥a s·ªë th·ª© t·ª± ·ªü ƒë·∫ßu (VD: "1. ", "10. ")
    """
    if not text:
        return ""
    
    # X√≥a kho·∫£ng tr·∫Øng 2 ƒë·∫ßu
    text = text.strip()
    
    # D√πng Regex x√≥a s·ªë th·ª© t·ª± ƒë·∫ßu d√≤ng (VD: "1.", "2)", "01.")
    # ^\d+ : B·∫Øt ƒë·∫ßu b·∫±ng s·ªë
    # [\.\)] : Theo sau l√† d·∫•u ch·∫•m ho·∫∑c ngo·∫∑c ƒë∆°n
    # \s* : Kho·∫£ng tr·∫Øng b·∫•t k·ª≥
    text = re.sub(r'^\d+[\.\)]\s*', '', text)
    
    return text

def crawl_cadao_tucngu(url, output_file):
    print(f"ƒêang t·∫£i d·ªØ li·ªáu t·ª´: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status() # B√°o l·ªói n·∫øu link ch·∫øt
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # T√¨m container ch·ª©a n·ªôi dung theo class b·∫°n cung c·∫•p
        # L∆∞u √Ω: Class trong HTML c√≥ th·ªÉ c√≥ nhi·ªÅu, ta ch·ªâ c·∫ßn t√¨m div c√≥ ch·ª©a class ch√≠nh
        content_div = soup.find('div', class_='content-detail')
        
        if not content_div:
            print("‚ùå Kh√¥ng t√¨m th·∫•y th·∫ª div ch·ª©a n·ªôi dung (class='content-detail').")
            return

        data_list = []
        seen_content = set() # D√πng ƒë·ªÉ l·ªçc tr√πng l·∫∑p
        current_id = 1

        # --- PH·∫¶N 1: L·∫•y d·ªØ li·ªáu t·ª´ th·∫ª <li> (th∆∞·ªùng l√† t·ª•c ng·ªØ ng·∫Øn) ---
        list_items = content_div.find_all('li')
        for li in list_items:
            text = li.get_text(strip=True)
            cleaned_text = clean_text(text)
            
            if cleaned_text and cleaned_text not in seen_content:
                data_list.append({
                    "id": current_id,
                    "content": cleaned_text,
                    "type": "li_tag" # (T√πy ch·ªçn) ƒê·ªÉ bi·∫øt ngu·ªìn g·ªëc
                })
                seen_content.add(cleaned_text)
                current_id += 1

        # --- PH·∫¶N 2: L·∫•y d·ªØ li·ªáu t·ª´ th·∫ª <p> (th∆∞·ªùng l√† ca dao/th∆°) ---
        p_items = content_div.find_all('p')
        for p in p_items:
            # X·ª≠ l√Ω th·∫ª <br> th√†nh xu·ªëng d√≤ng \n tr∆∞·ªõc khi l·∫•y text
            for br in p.find_all("br"):
                br.replace_with("\n")
            
            text = p.get_text()
            cleaned_text = clean_text(text)
            
            # L·ªçc b·ªõt c√°c d√≤ng r√°c (qu√° ng·∫Øn ho·∫∑c l√† ti√™u ƒë·ªÅ b√†i vi·∫øt)
            if cleaned_text and len(cleaned_text) > 5 and cleaned_text not in seen_content:
                # Ki·ªÉm tra th√™m: N·∫øu th·∫ª p ch·ªâ ch·ª©a th√¥ng tin metadata r√°c th√¨ b·ªè qua
                if "Ngu·ªìn:" in cleaned_text or "S∆∞u t·∫ßm" in cleaned_text:
                    continue
                    
                data_list.append({
                    "id": current_id,
                    "content": cleaned_text,
                    "type": "p_tag"
                })
                seen_content.add(cleaned_text)
                current_id += 1

        # L∆∞u ra file JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ ƒê√£ c√†o th√†nh c√¥ng {len(data_list)} c√¢u.")
        print(f"üìÅ K·∫øt qu·∫£ l∆∞u t·∫°i: {output_file}")

    except Exception as e:
        print(f"‚ùå C√≥ l·ªói x·∫£y ra: {e}")

# --- CH·∫†Y CH∆Ø∆†NG TR√åNH ---
if __name__ == "__main__":
    TARGET_URL = "http://thcamlinh.edu.vn/tin-tuc-su-kien/tin-cua-truong/500-cau-ca-dao-tuc-ngu-thanh-ngu-viet-nam-hay.html"
    OUTPUT_FILE = "./cadao_tucngu.json"
    
    crawl_cadao_tucngu(TARGET_URL, OUTPUT_FILE)