# -*- coding: utf-8 -*-
"""Qdrant_Hybrid_Search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18fn_YvK6TwQlQl8trTVrJAaoSFiltYfS

# Qdrant Essentials: Day 3 - Building Hybrid Search in Qdrant

Let's see how hybrid search might be implemented with Qdrant's Universal Query API.

## Step 1: Install the dependencies
"""

!pip install -q qdrant-client[fastembed]

"""## Step 2: Connect to Qdrant

Let's connect to a running [Qdrant Cloud](https://cloud.qdrant.io/) cluster and create a collection containing both sparse and dense named vectors.
"""

from qdrant_client import QdrantClient
from google.colab import userdata

client = QdrantClient(
    location="https://9d345c5f-d76f-48e2-b516-db6ea2e76b74.us-west-1-0.aws.cloud.qdrant.io",
    api_key=userdata.get("api-key")
)

from qdrant_client import models

# Define the collection name
collection_name = "hybrid_search_demo"

# Create our collection with both sparse (bm25) and dense vectors
client.create_collection(
    collection_name=collection_name,
    vectors_config={
        "dense": models.VectorParams(
            distance=models.Distance.COSINE,
            size=384,
        ),
    },
    sparse_vectors_config={
        "sparse": models.SparseVectorParams(
            modifier=models.Modifier.IDF
        )
    }
)

"""## Step 3: Upload the data into the collection

Now, we have a collection that allows us to store two vectors per point, and we can finally fill it with data.
"""

documents = [
    "Aged Gouda develops a crystalline texture and nutty flavor profile after 18 months of maturation.",
    "Mature Gouda cheese becomes grainy and develops a rich, buttery taste with extended aging.",
    "Brie cheese features a soft, creamy interior surrounded by an edible white rind.",
    "This French cheese has a flowing, buttery center encased in a bloomy white crust.",
    "Fresh mozzarella pairs beautifully with ripe tomatoes and basil leaves.",
    "Classic Margherita pizza topped with tomato sauce, mozzarella, and fresh basil.",
    "Parmesan requires at least 12 months of cave aging to develop its signature sharp taste.",
    "Parmigiano-Reggiano's distinctive piquant flavor comes from extended maturation in controlled environments.",
    "Grilled cheese sandwiches are the ultimate American comfort food for cold winter days.",
    "Croque Monsieur combines ham and Gruyère in France's answer to the toasted cheese sandwich.",
]

import uuid

client.upsert(
    collection_name=collection_name,
    points=[
        models.PointStruct(
            id=uuid.uuid4().hex,
            vector={
                "dense": models.Document(
                    text=doc,
                    model="sentence-transformers/all-MiniLM-L6-v2",
                ),
                "sparse": models.Document(
                    text=doc,
                    model="Qdrant/bm25",
                ),
            },
            payload={"text": doc},
        )
        for i, doc in enumerate(documents)
    ]
)

from fastembed import TextEmbedding

def list_fastembed_models():
    # Lấy danh sách các mô hình được hỗ trợ
    models = TextEmbedding.list_supported_models()

    print(f"{'Tên Mô Hình':<50} | {'Dim':<5} | {'Mô tả':<20}")
    print("-" * 85)

    for model in models:
        # Trích xuất thông tin quan trọng
        name = model['model']
        dim = model['dim']
        description = model['description']

        print(f"{name:<50} | {dim:<5} | {description:<20}")

if __name__ == "__main__":
    print("Danh sách các mô hình Dense Embedding được FastEmbed hỗ trợ:\n")
    list_fastembed_models()

from fastembed import SparseTextEmbedding

def list_sparse_models():
    models = SparseTextEmbedding.list_supported_models()

    print(f"{'Tên Mô Hình Sparse':<50} | {'Mô tả'}")
    print("-" * 80)

    for model in models:
        name = model['model']
        desc = model['description']
        print(f"{name:<50} | {desc}")

if __name__ == "__main__":
    print("\nDanh sách các mô hình Sparse Embedding:\n")
    list_sparse_models()

"""## Step 4: Validating the outputs of sparse and dense search

Both of our models may return completely different sets of results for the same query. Let's check if that's the case.
"""

def dense_search(query: str) -> list[models.ScoredPoint]:
    response = client.query_points(
        collection_name=collection_name,
        query=models.Document(
            text=query,
            model="sentence-transformers/all-MiniLM-L6-v2",
        ),
        using="dense",
        limit=3,
    )
    return response.points

def sparse_search(query: str) -> list[models.ScoredPoint]:
    response = client.query_points(
        collection_name=collection_name,
        query=models.Document(
            text=query,
            model="Qdrant/bm25",
        ),
        using="sparse",
        limit=3,
    )
    return response.points

"""Let's run both methods on some of the possible queries, to see if the outputs really differ."""

queries = [
    "nutty aged cheese",
    "soft French cheese",
    "pizza ingredients",
    "a good lunch",
]

for query in queries:
    print("Query:", query)

    dense_results = dense_search(query)
    print("Dense Results:")
    for result in dense_results:
        print("\t-", result.payload["text"], result.score)

    sparse_results = sparse_search(query)
    print("Sparse Results:")
    for result in sparse_results:
        print("\t-", result.payload["text"], result.score)
    print()

"""## Step 5: Hybrid Search with Reciprocal Rank Fusion

Scores coming from both methods are incompatible, but RRF will not use them directly. It will only consider the order / ranking of the elements, so let's implement such a hybrid search pipeline.
"""

def rrf_search(query: str) -> list[models.ScoredPoint]:
    response = client.query_points(
        collection_name=collection_name,
        prefetch=[
            models.Prefetch(
                query=models.Document(
                    text=query,
                    model="Qdrant/bm25",
                ),
                using="sparse",
                limit=3,
            ),
            models.Prefetch(
                query=models.Document(
                    text=query,
                    model="sentence-transformers/all-MiniLM-L6-v2",
                ),
                using="dense",
                limit=3,
            )
        ],
        query=models.FusionQuery(fusion=models.Fusion.RRF),
        limit=3,
    )
    return response.points

for query in queries:
    print("Query:", query)

    rrf_results = rrf_search(query)
    print("RRF Results:")
    for result in rrf_results:
        print("\t-", result.payload["text"], result.score)
    print()

"""## Step 6: Distribution-Based Score Fusion

RRF is not the only supported fusion method. DBSF is another option that normalizes the scores of the points in each query, and sums the scores of the same point across different queries. Choosing a different algorithm can definitely impact the final outputs, so let's see how they are going to look like.
"""

def dbsf_search(query: str) -> list[models.ScoredPoint]:
    response = client.query_points(
        collection_name=collection_name,
        prefetch=[
            models.Prefetch(
                query=models.Document(
                    text=query,
                    model="Qdrant/bm25",
                ),
                using="sparse",
                limit=3,
            ),
            models.Prefetch(
                query=models.Document(
                    text=query,
                    model="sentence-transformers/all-MiniLM-L6-v2",
                ),
                using="dense",
                limit=3,
            )
        ],
        query=models.FusionQuery(fusion=models.Fusion.DBSF),
        limit=3,
    )
    return response.points

for query in queries:
    print("Query:", query)

    dbsf_results = dbsf_search(query)
    print("DBSF Results:")
    for result in dbsf_results:
        print("\t-", result.payload["text"], result.score)
    print()